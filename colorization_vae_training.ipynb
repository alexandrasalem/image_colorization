{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colorization_vae_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXv9dyDqf2yQ"
      },
      "source": [
        "Training notebook for:\n",
        "\n",
        "Model 3: VAE without fusion\n",
        "\n",
        "Model 4: VAE with fusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-MwzQWvJ4XE",
        "outputId": "feb76b2f-a15d-4aab-e017-20dbc546cb31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nRKmYPfb9r"
      },
      "source": [
        "Loading libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "506DQHMTUA4q"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from skimage.io import imsave, imshow, imread\n",
        "from skimage.color import rgb2lab, lab2rgb, grey2rgb, rgb2grey\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input, decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv5ilz0iGfN4",
        "outputId": "42c83b31-e0ae-4373-d52b-f990012daa9d"
      },
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "225214464/225209952 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aXf0F60feDO"
      },
      "source": [
        "Data loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pajj-KV7yRCW"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates data for Keras\n",
        "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_IDs, image_path,\n",
        "                 to_fit=True, batch_size=32, dim=(256, 256), shuffle=True, fusion=False, fusion_path='fusion/'):\n",
        "        \"\"\"Initialization\n",
        "\n",
        "        :param list_IDs: list of all 'label' ids to use in the generator\n",
        "        :param image_path: path to images location\n",
        "        :param to_fit: True to return X and y, False to return X only\n",
        "        :param batch_size: batch size at each iteration\n",
        "        :param dim: tuple indicating image dimension\n",
        "        :param shuffle: True to shuffle label indexes after every epoch\n",
        "        :param fusion: True to return X and X_fusion, False returns X\n",
        "        \"\"\"\n",
        "        self.list_IDs = list_IDs\n",
        "        self.image_path = image_path\n",
        "        self.to_fit = to_fit\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.shuffle = shuffle\n",
        "        self.fusion = fusion\n",
        "        self.fusion_path = fusion_path\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\n",
        "\n",
        "        :return: number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\n",
        "\n",
        "        :param index: index of the batch\n",
        "        :return: X and y when fitting. X only when predicting\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X = self._generate_X(list_IDs_temp)\n",
        "        if self.to_fit:\n",
        "            y = self._generate_y(list_IDs_temp)\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\n",
        "\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def _generate_X(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size images\n",
        "\n",
        "        :param list_IDs_temp: list of label ids to load\n",
        "        :return: batch of images\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, 1))\n",
        "        if self.fusion:\n",
        "          X_fusion = np.empty((self.batch_size, 1000))\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            X[i,] = self._load_lab_grayscale_image(self.image_path + ID)\n",
        "            if self.fusion:\n",
        "              X_fusion[i,] = self._load_fusion(self.fusion_path + ID)\n",
        "\n",
        "        if self.fusion:\n",
        "          return (X, X_fusion)\n",
        "        else:\n",
        "          return X\n",
        "\n",
        "    def _generate_y(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size masks\n",
        "\n",
        "        :param list_IDs_temp: list of label ids to load\n",
        "        :return: batch if masks\n",
        "        \"\"\"\n",
        "        y = np.empty((self.batch_size, *self.dim, 2))\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            y[i,] = self._load_lab_color_image(self.image_path + ID)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def _load_lab_grayscale_image(self, image_path):\n",
        "      img = imread(image_path)\n",
        "      img = img*(1.0/255)\n",
        "      img = resize(img, (256, 256))\n",
        "      if img.shape == (256, 256):\n",
        "        img = grey2rgb(img)\n",
        "      img = rgb2lab(img)\n",
        "      img =(img[:,:,0]).reshape(img[:,:,0].shape+(1,))\n",
        "      return img\n",
        "\n",
        "    def _load_fusion(self, image_path):\n",
        "      with open(image_path, 'rb') as f:\n",
        "          embed = np.load(f)\n",
        "          return embed \n",
        "\n",
        "    def _load_lab_color_image(self, image_path):\n",
        "      img = imread(image_path)\n",
        "      img = img*(1.0/255)\n",
        "      img = resize(img, (256, 256))\n",
        "      if img.shape == (256, 256):\n",
        "        img = grey2rgb(img)\n",
        "      img = rgb2lab(img)\n",
        "      img = img[:,:,1:]*(1.0/128)\n",
        "      return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG2g6-uSfgOQ"
      },
      "source": [
        "Sampling layer for VAE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4dUwPnDUEDw"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN4sjgABfiGi"
      },
      "source": [
        "Model 4 VAE with fusion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpVA3c5EUNdE"
      },
      "source": [
        "#VAE with fusion\n",
        "latent_dim = 256\n",
        "initializer = tf.keras.initializers.Zeros()\n",
        "\n",
        "encoder_input = layers.Input(\n",
        "    shape=(256, 256, 1,), name=\"input\"\n",
        ") \n",
        "embed_input = layers.Input(shape=(1000,))\n",
        "\n",
        "encoder_output = layers.Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Flatten()(encoder_output)\n",
        "encoder_output = layers.Dense(128, activation=\"relu\")(encoder_output)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoder_output)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer=initializer)(encoder_output)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "z = layers.concatenate([z, embed_input], axis=1)\n",
        "\n",
        "encoder = keras.Model([encoder_input, embed_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim +1000,)) #1256 dim vector\n",
        "decoder_output = layers.Dense(32 * 32 * 128, activation=\"relu\")(latent_inputs)\n",
        "decoder_output = layers.Reshape((32, 32, 128))(decoder_output)\n",
        "decoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder = keras.Model(latent_inputs, decoder_output, name=\"decoder\")\n",
        "\n",
        "class VAEFusion(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAEFusion, self).__init__(**kwargs)\n",
        "        print(\"in init\")\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        print(\"set encoder and decoder\")\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "        #self.accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "            #self.accuracy,\n",
        "            \n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # adapt to taking in the dataloader\n",
        "        print(\"in train\")\n",
        "        print(data)\n",
        "        input, hat = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            # pull out x from data, feed into encoder and decoder\n",
        "            z_mean, z_log_var, z = self.encoder(input)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mse(hat, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + .01*kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        #self.accuracy.update_state(hat, reconstruction)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            #\"accuracy\": self.accuracy.result(),\n",
        "        }\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        input, hat = data\n",
        "        # Compute predictions\n",
        "        z_mean, z_log_var, z = self.encoder(input)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.mse(hat, reconstruction), axis=(1, 2)\n",
        "            )\n",
        "        )\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        total_loss = reconstruction_loss + 0.01*kl_loss\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        #self.accuracy.update_state(hat, reconstruction)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            #\"accuracy\": self.accuracy.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, data):\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z) \n",
        "        return reconstruction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iCGq3ijfmJx"
      },
      "source": [
        "Model 3 VAE without fusion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrJ-n0DPyFjP"
      },
      "source": [
        "# VAE without fusion\n",
        "latent_dim = 256\n",
        "initializer = tf.keras.initializers.Zeros()\n",
        "\n",
        "encoder_input = layers.Input(\n",
        "    shape=(256, 256, 1,), name=\"input\"\n",
        ") \n",
        "\n",
        "encoder_output = layers.Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = layers.Flatten()(encoder_output)\n",
        "encoder_output = layers.Dense(128, activation=\"relu\")(encoder_output)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoder_output)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer=initializer)(encoder_output)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "decoder_output = layers.Dense(32 * 32 * 128, activation=\"relu\")(latent_inputs)\n",
        "decoder_output = layers.Reshape((32, 32, 128))(decoder_output)\n",
        "decoder_output = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = layers.Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = layers.UpSampling2D((2, 2))(decoder_output)\n",
        "decoder = keras.Model(latent_inputs, decoder_output, name=\"decoder\")\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # adapt to taking in the dataloader\n",
        "        input, hat = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(input)\n",
        "            reconstruction = self.decoder(z)\n",
        "            print(z)\n",
        "            #hat = data[:,:,:,1:3]/128\n",
        "            print(hat)\n",
        "\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mse(hat, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + .01*kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        input, hat = data\n",
        "        # Compute predictions\n",
        "        z_mean, z_log_var, z = self.encoder(input)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.mse(hat, reconstruction), axis=(1, 2)\n",
        "            )\n",
        "        )\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        total_loss = reconstruction_loss + 0.01*kl_loss\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, data):\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z) \n",
        "        return reconstruction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sknmVJCTfpS6"
      },
      "source": [
        "Loading data list, making inception-resnet-v2 representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdGE_HSzZQ_4"
      },
      "source": [
        "with open(\"places_sample_train.txt\") as f:\n",
        "  train_list = f.readlines()\n",
        "train_list = [x[:-1] for x in train_list]\n",
        "\n",
        "with open(\"places_sample_val.txt\") as f:\n",
        "  val_list = f.readlines()\n",
        "val_list = [x[:-1] for x in val_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kTNnJTgHk8r",
        "outputId": "7795f006-5915-4d93-ccf2-e8ddede13fe1"
      },
      "source": [
        "!mkdir fusion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘fusion’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWD4ZwRJErVl"
      },
      "source": [
        "for image in val_list:\n",
        "      img = imread('./gdrive/MyDrive/val_256/'+ image)\n",
        "      img = img*(1.0/255)\n",
        "      img = grey2rgb(rgb2grey(img))\n",
        "      img = resize(img, (299, 299))\n",
        "      img = preprocess_input(img)\n",
        "      img = img.reshape((1,) + img.shape)\n",
        "      embed = inception.predict(img)\n",
        "      with open('./fusion/' + image, 'wb') as f:\n",
        "          np.save(f, embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_jE34hEFY0v"
      },
      "source": [
        "for image in train_list:\n",
        "      img = imread('./gdrive/MyDrive/val_256/'+ image)\n",
        "      img = img*(1.0/255)\n",
        "      img = grey2rgb(rgb2grey(img))\n",
        "      img = resize(img, (299, 299))\n",
        "      img = preprocess_input(img)\n",
        "      img = img.reshape((1,) + img.shape)\n",
        "      embed = inception.predict(img)\n",
        "      with open('./fusion/' + image, 'wb') as f:\n",
        "          np.save(f, embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLmLcn9VfvR2"
      },
      "source": [
        "Training model 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZptKoRYFpiH"
      },
      "source": [
        "image_path = \"gdrive/MyDrive/val_256/\"\n",
        "train_datagen = DataGenerator(train_list, image_path, fusion = True, batch_size = 100)\n",
        "val_datagen = DataGenerator(val_list, image_path, fusion = True, batch_size = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIvkfPRZFsSL"
      },
      "source": [
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 7)\n",
        "mc = keras.callbacks.ModelCheckpoint('./gdrive/MyDrive/colorize_vae_fusion_10000_es.model', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "vae = VAEFusion(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(train_datagen, validation_data = val_datagen, epochs=100, callbacks = [es, mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKxVOPZWfw7n"
      },
      "source": [
        "Training model 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPaaS2yQKRAf"
      },
      "source": [
        "image_path = \"gdrive/MyDrive/val_256/\"\n",
        "train_datagen = DataGenerator(train_list, image_path, fusion = False, batch_size = 100)\n",
        "val_datagen = DataGenerator(val_list, image_path, fusion = False, batch_size = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z7Uh0ZKFPuO"
      },
      "source": [
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 7)\n",
        "mc = keras.callbacks.ModelCheckpoint('other_files/colorize_vae_10000_es.model', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(train_datagen, validation_data = val_datagen, epochs=100, callbacks=[es, mc])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}